{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 12:15:03.058244: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-30 12:15:03.911685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.analyzation_tools import corrected_repeated_kFold_cv_test as cv_test\n",
    "from finetuning.model.region_loss import Regional_Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_loss(list_of_df, name, save_path):\n",
    "    \"\"\"\n",
    "    Function to compare metrics of different experiments using t-tests.\n",
    "    :param list_of_df: List of dataframes\n",
    "    :param name: Name of the experiment\n",
    "    :param save_path: Path to save the plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    list_of_df = [df.copy() for df in list_of_df]\n",
    "    for i in range(len(list_of_df)):\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=f\"L{i+1}\")\n",
    "        cols_to_drop= list_of_df[i].filter(like='text', axis=1).columns\n",
    "        list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "        \n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    metrics = condf.columns[:-1]\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if type(x) == list else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].astype(float)\n",
    "    loss_config = ['L1', 'L2', 'L3', 'L4']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        matrix = []\n",
    "        print(f\"Metric: {metric}\")\n",
    "        for i in range(len(loss_config)):\n",
    "            buffer = []\n",
    "            for j in range(len(loss_config)):\n",
    "                exp1 = loss_config[i]\n",
    "                exp2 = loss_config[j]\n",
    "                \n",
    "                values1 = meltdf[meltdf['Experiment'] == exp1]\n",
    "                values1 = values1[values1['Metric'] == metric]['Value']\n",
    "                values2 = meltdf[meltdf['Experiment'] == exp2]\n",
    "                values2 = values2[values2['Metric'] == metric]['Value']\n",
    "                # We assume significnce level of 0.05; due to 10 flod validation we have 9:1 ration of samples\n",
    "                _,_,_, p_value = cv_test(values1.to_list(), values2.to_list(), 9, 1, 0.05)\n",
    "                if p_value < 0.05:\n",
    "                    if values1.mean() > values2.mean():\n",
    "                        print(f\"{exp1} is significantly better than {exp2}\")\n",
    "                        buffer.append(1)\n",
    "                    else:\n",
    "                        print(f\"{exp2} is significantly better than {exp1}\")\n",
    "                        buffer.append(-1)\n",
    "                else:\n",
    "                    print(f\"No significant difference between {exp1} and {exp2}\")\n",
    "                    buffer.append(0)\n",
    "            matrix.append(buffer)\n",
    "        matrix = pd.DataFrame(matrix, index=loss_config, columns=loss_config)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(matrix, annot=True, cmap='coolwarm', cbar=False)\n",
    "        plt.title(f\"{metric} comparison\")\n",
    "        plt.savefig(save_path + f\"{name}_{metric}_comparison.png\")\n",
    "        plt.close()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(df, data_type,REPO_PATH):\n",
    "    \"\"\"\n",
    "    Calculate the metrics for region and country columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the metrics.\n",
    "        data_type (str): The type of data (validation or test).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    country_list = f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv'\n",
    "    country_list = pd.read_csv(country_list)\n",
    "    metrics_calculator = Regional_Loss(country_list=country_list)\n",
    "    # Convert the 'Output' column to a list of tensors\n",
    "    df['Output'] = df['Output'].apply(lambda x: torch.tensor(x))\n",
    "\n",
    "    # Stack the list of tensors into a single tensor\n",
    "    outputs = torch.stack(df['Output'].tolist())\n",
    "    if data_type == 'validation':\n",
    "        NotImplementedError\n",
    "    elif data_type == 'test':\n",
    "        c_ac = metrics_calculator.calculate_country_accuracy(outputs, df['Label'])\n",
    "        c_prec, c_rec, c_f1,_,_ = metrics_calculator.calculate_metrics_per_class(outputs, df['Label'])\n",
    "        r_ac = metrics_calculator.claculate_region_accuracy(outputs, df['Label'])\n",
    "        r_prec, r_rec, r_f1,_,_ = metrics_calculator.calculate_metrics_per_region(outputs, df['Label'])\n",
    "        ignored_class = len(df['Label'].unique())  - len(df['Prediction'].unique())\n",
    "    metrics = {\n",
    "        'country_accuracy': [c_ac],\n",
    "        'country_precision': [c_prec.mean()],\n",
    "        'country_recall': [c_rec.mean()],\n",
    "        'country_f1': [c_f1.mean()],\n",
    "        'region_accuracy': [r_ac],\n",
    "        'region_precision': [r_prec.mean()],\n",
    "        'region_recall': [r_rec.mean()],\n",
    "        'region_f1': [r_f1.mean()],\n",
    "        'ignored_classes': [ignored_class]\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_dir(log_dir, REPO_PATH):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the dataframes\n",
    "    validation_dfs = []\n",
    "    test_dfs = []\n",
    "    zero_shot_dfs = []\n",
    "\n",
    "    # Iterate over the folders in the log directory\n",
    "    for folder in sorted(os.listdir(log_dir)):\n",
    "        folder_path = os.path.join(log_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # calculate the metrics for all seeds in the folder\n",
    "            log_files = glob.glob(folder_path + \"/*\")\n",
    "            validation_buffer = []\n",
    "            test_buffer = []\n",
    "            zero_shot_buffer = []\n",
    "            for file_path in log_files:\n",
    "                if '.csv' not in file_path:\n",
    "                    continue\n",
    "                df = pd.read_csv(file_path,converters={\"Output\": ast.literal_eval})\n",
    "\n",
    "                # Split the data into validation and test data\n",
    "                if 'validation' in file_path:\n",
    "                    df = calculate_metrics(df, 'validation', REPO_PATH=REPO_PATH)\n",
    "                    validation_buffer.append(df)\n",
    "                elif 'test' in file_path:\n",
    "                    df = calculate_metrics(df, 'test', REPO_PATH=REPO_PATH)\n",
    "                    test_buffer.append(df)\n",
    "                elif 'zero' in file_path:\n",
    "                    df = calculate_metrics(df, 'zero', REPO_PATH=REPO_PATH)\n",
    "                    zero_shot_buffer.append(df)\n",
    "            validation_dfs.append(pd.concat(validation_buffer))\n",
    "            test_dfs.append(pd.concat(test_buffer))\n",
    "            zero_shot_dfs.append(pd.concat(zero_shot_buffer))\n",
    "    return validation_dfs, test_dfs, zero_shot_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_PATH = '/home/leon/Documents/GPML/good_practices_ml/'\n",
    "\n",
    "# directory of all experiments\n",
    "experimient_dir= '/media/leon/Samsung_T5/Uni/good_practices_ml/runs/merged_seeds/'\n",
    "# create lists that contain the dataframes of the different experiments\n",
    "# First axis contains the different dataset configurations\n",
    "# Second axis contains the different Loss configurations\n",
    "# Third axis contains the DataFrame of the different seeds\n",
    "validation_sets = []\n",
    "test_sets = []\n",
    "zeros_shot_datasets = []\n",
    "\n",
    "\n",
    "for folder in sorted(os.listdir(experimient_dir)):\n",
    "    log_dir = os.path.join(experimient_dir, folder)\n",
    "    if os.path.isdir(log_dir):\n",
    "        save_path = log_dir + '/results/'\n",
    "        # Call the event_to_df function with the log directory \n",
    "        val, test, zero= read_csv_from_dir(log_dir,REPO_PATH)\n",
    "        validation_sets.append(val)\n",
    "        test_sets.append(test)\n",
    "        zeros_shot_datasets.append(zero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = ['Strongly Balanced', 'Unbalanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Unbalanced', 'Mixed Weakly Balanced']\n",
    "save_path = '/media/leon/Samsung_T5/Uni/good_practices_ml/runs/merged_seeds/figures/'\n",
    "for experiment in enumerate(validation_sets):\n",
    "    name = dataset_config[experiment[0]]\n",
    "    compare_loss(experiment, name, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
