{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 09:44:38.945846: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-03 09:44:41.128336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.analyzation_tools import corrected_repeated_kFold_cv_test as cv_test\n",
    "from finetuning.model.region_loss import Regional_Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_loss(list_of_df, name, save_path):\n",
    "    \"\"\"\n",
    "    Function to compare metrics of different experiments using t-tests.\n",
    "    :param list_of_df: List of dataframes\n",
    "    :param name: Name of the experiment\n",
    "    :param save_path: Path to save the plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    list_of_df = [df.copy() for df in list_of_df]\n",
    "    for i in range(len(list_of_df)):\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=f\"L{i+1}\")\n",
    "        cols_to_drop= list_of_df[i].filter(like='text', axis=1).columns.tolist()\n",
    "        cols_to_drop.extend(['prediction', 'label'])\n",
    "        list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "        \n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    metrics = condf.columns[:-1]\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: x.item() if isinstance(x, torch.Tensor) else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if isinstance(x, list) else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].astype(float)\n",
    "    loss_config = ['L1', 'L2', 'L3', 'L4']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        matrix = []\n",
    "        print(f\"Metric: {metric}\")\n",
    "        for i in range(len(loss_config)):\n",
    "            buffer = []\n",
    "            for j in range(len(loss_config)):\n",
    "                exp1 = loss_config[i]\n",
    "                exp2 = loss_config[j]\n",
    "                \n",
    "                values1 = meltdf[meltdf['Experiment'] == exp1]\n",
    "                values1 = values1[values1['Metric'] == metric]['Value']\n",
    "                values2 = meltdf[meltdf['Experiment'] == exp2]\n",
    "                values2 = values2[values2['Metric'] == metric]['Value']\n",
    "                # We assume significnce level of 0.05; due to 10 flod validation we have 9:1 ration of samples\n",
    "                t_stat, p_value = ttest_ind(values1.to_list(), values2.to_list())\n",
    "                if p_value < 0.05:\n",
    "                    if values1.mean() > values2.mean():\n",
    "                        print(f\"{exp1} is significantly better than {exp2}\")\n",
    "                        buffer.append(1)\n",
    "                    else:\n",
    "                        print(f\"{exp2} is significantly better than {exp1}\")\n",
    "                        buffer.append(-1)\n",
    "                else:\n",
    "                    print(f\"No significant difference between {exp1} and {exp2}\")\n",
    "                    buffer.append(0)\n",
    "            matrix.append(buffer)\n",
    "        matrix = pd.DataFrame(matrix, index=loss_config, columns=loss_config)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(matrix, annot=True, cmap='coolwarm', cbar=False)\n",
    "        plt.title(f\"{metric} comparison\")\n",
    "        plt.savefig(save_path + f\"{name}_{metric}_comparison.png\")\n",
    "        plt.close()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_dataframe(df_data1, df_data2, dataset_names):\n",
    "    \"\"\"\n",
    "    Function to compare metrics of different experiments using t-tests.\n",
    "    :param list_of_df: List of dataframes\n",
    "    :param name: Name of the experiment\n",
    "    :param save_path: Path to save the plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    cols_to_drop = df_data1.filter(like='text', axis=1).columns\n",
    "    df_data1 = df_data1.drop(columns=cols_to_drop)\n",
    "    df_data2 = df_data2.drop(columns=cols_to_drop)\n",
    "    df_data1.columns = df_data1.columns.str.split().str[-2:].str.join(\" \")\n",
    "    df_data2.columns = df_data2.columns.str.split().str[-2:].str.join(\" \")\n",
    "    metrics = df_data1.columns[:-1]\n",
    "        \n",
    "    for metric in metrics:\n",
    "        print(f\"Metric: {metric}\")\n",
    "        data1 = df_data1[metric]\n",
    "        data2 = df_data2[metric]\n",
    "        t_stat, p_value = ttest_ind(data1.to_list(), data2.to_list())\n",
    "        if p_value < 0.05:\n",
    "            if data1.mean() > data2.mean():\n",
    "                print(f\"{dataset_names[0]} is significantly better than {dataset_names[1]}\")\n",
    "            else:\n",
    "                print(f\"{dataset_names[1]} is significantly better than {dataset_names[0]}\")\n",
    "        else:\n",
    "            print(f\"No significant difference between {dataset_names[0]} and {dataset_names[1]}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(df, data_type, REPO_PATH):\n",
    "    \"\"\"\n",
    "    Calculate the metrics for region and country columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the metrics.\n",
    "        data_type (str): The type of data (validation or test).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    country_list = f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv'\n",
    "    country_list = pd.read_csv(country_list)\n",
    "    metrics_calculator = Regional_Loss(country_list=country_list)\n",
    "    # Convert the 'Output' column to a list of tensors\n",
    "    df['Output'] = df['Output'].apply(lambda x: torch.tensor(x))\n",
    "    if data_type == 'validation':\n",
    "        dfs = np.array_split(df, 10)\n",
    "        for df in dfs:\n",
    "            # Stack the list of tensors into a single tensor\n",
    "            outputs = torch.stack(df['Output'].tolist())\n",
    "            c_ac = metrics_calculator.calculate_country_accuracy(outputs, df['Label'])\n",
    "            c_prec, c_rec, c_f1,_,_ = metrics_calculator.calculate_metrics_per_class(outputs, df['Label'])\n",
    "            r_ac = metrics_calculator.claculate_region_accuracy(outputs, df['Label'])\n",
    "            r_prec, r_rec, r_f1,_,_ = metrics_calculator.calculate_metrics_per_region(outputs, df['Label'])\n",
    "            ignored_class = len(df['Label'].unique())  - len(df['Prediction'].unique())\n",
    "            ignored_region= sum(1 for x, y in zip(r_prec, r_rec) if x == 0 and y == 0)\n",
    "\n",
    "            metrics = {\n",
    "                'country_accuracy': [c_ac],\n",
    "                'country_precision': [c_prec.mean()],\n",
    "                'country_recall': [c_rec.mean()],\n",
    "                'country_f1': [c_f1.mean()],\n",
    "                'region_accuracy': [r_ac],\n",
    "                'region_precision': [r_prec.mean()],\n",
    "                'region_recall': [r_rec.mean()],\n",
    "                'region_f1': [r_f1.mean()],\n",
    "                'ignored_classes': [ignored_class],\n",
    "                'ignored_regions': [ignored_region],\n",
    "                'prediction': [df['Prediction']],\n",
    "                'label': [df['Label']]\n",
    "            }\n",
    "            if 'all_metrics' in locals():\n",
    "                all_metrics = pd.concat([all_metrics, pd.DataFrame(metrics)])\n",
    "            else:\n",
    "                all_metrics = pd.DataFrame(metrics)\n",
    "    else:\n",
    "        # Stack the list of tensors into a single tensor\n",
    "        outputs = torch.stack(df['Output'].tolist())\n",
    "        c_ac = metrics_calculator.calculate_country_accuracy(outputs, df['Label'])\n",
    "        c_prec, c_rec, c_f1,_,_ = metrics_calculator.calculate_metrics_per_class(outputs, df['Label'])\n",
    "        r_ac = metrics_calculator.claculate_region_accuracy(outputs, df['Label'])\n",
    "        r_prec, r_rec, r_f1,_,_ = metrics_calculator.calculate_metrics_per_region(outputs, df['Label'])\n",
    "        ignored_class = len(df['Label'].unique())  - len(df['Prediction'].unique())\n",
    "        ignored_region= sum(1 for x, y in zip(r_prec, r_rec) if x == 0 and y == 0)\n",
    "\n",
    "        metrics = {\n",
    "            'country_accuracy': [c_ac],\n",
    "            'country_precision': [c_prec.mean()],\n",
    "            'country_recall': [c_rec.mean()],\n",
    "            'country_f1': [c_f1.mean()],\n",
    "            'region_accuracy': [r_ac],\n",
    "            'region_precision': [r_prec.mean()],\n",
    "            'region_recall': [r_rec.mean()],\n",
    "            'region_f1': [r_f1.mean()],\n",
    "            'ignored_classes': [ignored_class],\n",
    "            'ignored_regions': [ignored_region],\n",
    "            'prediction': [df['Prediction']],\n",
    "            'label': [df['Label']]\n",
    "        }\n",
    "        all_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_dir(log_dir, REPO_PATH):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the dataframes\n",
    "    validation_dfs = []\n",
    "    test_dfs = []\n",
    "    zero_shot_dfs = []\n",
    "\n",
    "    # Iterate over the folders in the log directory\n",
    "    for folder in sorted(os.listdir(log_dir)):\n",
    "        folder_path = os.path.join(log_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # calculate the metrics for all seeds in the folder\n",
    "            log_files = glob.glob(folder_path + \"/*\")\n",
    "            validation_buffer = []\n",
    "            test_buffer = []\n",
    "            zero_shot_buffer = []\n",
    "            for file_path in log_files:\n",
    "                if '.csv' not in file_path:\n",
    "                    continue\n",
    "                df = pd.read_csv(file_path,converters={\"Output\": ast.literal_eval})\n",
    "\n",
    "                # Split the data into validation and test data\n",
    "                #if 'validation' in file_path:\n",
    "                #    df = calculate_metrics(df, 'validation', REPO_PATH=REPO_PATH)\n",
    "                #    validation_buffer.append(df)\n",
    "                if 'zero' in file_path:\n",
    "                    df = calculate_metrics(df, 'zero', REPO_PATH=REPO_PATH)\n",
    "                    zero_shot_buffer.append(df)\n",
    "                elif 'test' in file_path:\n",
    "                    df = calculate_metrics(df, 'test', REPO_PATH=REPO_PATH)\n",
    "                    test_buffer.append(df)\n",
    "            #validation_dfs.append(pd.concat(validation_buffer))\n",
    "            test_dfs.append(pd.concat(test_buffer))\n",
    "            zero_shot_dfs.append(pd.concat(zero_shot_buffer))\n",
    "    return validation_dfs, test_dfs, zero_shot_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_experiments(list_of_df, name, save_path, loss_number=0, dataset_names=None, metric_names=None, legend_out_of_plot=False):\n",
    "    \"\"\"\n",
    "    Genreates box plots for all metrics contained in the dataframes.\n",
    "    Compares these metrics for each dataframe in the list.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_df (list): A list of dataframes.\n",
    "    name (str): The name of the experiment.\n",
    "    save_path (str): The path to save the plot.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A concatenated dataframe containing all data with a coloumn tagging the used Loss.\n",
    "    \"\"\"\n",
    "    dataset_to_indices = {'Strongly Balanced':0, 'Unbalanced':1, 'Weakly Balanced':2, 'Mixed Strongly Balanced':3, 'Mixed Weakly Balanced':4}\n",
    "    if dataset_names is not None:\n",
    "        indices = [dataset_to_indices[name] for name in dataset_names]\n",
    "        list_of_df = [list_of_df[i] for i in indices]\n",
    "\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    list_of_df = [df[loss_number].copy() for df in list_of_df]\n",
    "\n",
    "    cols_to_use = metric_names.copy()\n",
    "    cols_to_use.append('Experiment')\n",
    "\n",
    "    for i in range(len(list_of_df)):\n",
    "\n",
    "\n",
    "        keys = list(dataset_to_indices.keys())\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=keys[indices[i]])\n",
    "        list_of_df[i] = list_of_df[i][cols_to_use]\n",
    "        #cols_to_drop = list_of_df[i].filter(like='text', axis=1).columns\n",
    "        #list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "\n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "        list_of_df[i] = list_of_df[i].rename(columns={'country_accuracy': 'Accuracy', 'country_precision': 'Precision', 'country_recall': 'Recall', 'country_f1': 'F1', 'region_accuracy': 'Accuracy', 'region_precision': 'Precision', 'region_recall': 'Recall', 'region_f1': 'F1'})\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if isinstance(x,list) else x) \n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x.item()) if isinstance(x,torch.Tensor) else x) \n",
    "\n",
    "    ax = sns.boxplot(\n",
    "        x=\"Metric\", y=\"Value\", hue=\"Experiment\", data=meltdf, showfliers=False\n",
    "    )\n",
    "    ax.set_title(name)\n",
    "    if legend_out_of_plot:\n",
    "        lgd = plt.legend(loc='upper left', fontsize='small', borderaxespad=0.0, bbox_to_anchor=(1, 1))\n",
    "    else:\n",
    "        lgd = plt.legend(loc='upper right', fontsize='small', borderaxespad=0.0)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.savefig(\n",
    "        save_path + f\"{name}-boxplot.png\",\n",
    "        bbox_extra_artists=(lgd,),\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return condf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/temp/bjordan/good_practices_in_machine_learning/good_practices_ml/venv_gpml/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "REPO_PATH = '/share/temp/bjordan/good_practices_in_machine_learning/good_practices_ml/'\n",
    "\n",
    "# directory of all experiments\n",
    "experimient_dir= '/share/temp/bjordan/good_practices_in_machine_learning/good_practices_ml/finetuning/runs/merged_seeds2/'\n",
    "# create lists that contain the dataframes of the different experiments\n",
    "# First axis contains the different dataset configurations\n",
    "# Second axis contains the different Loss configurations\n",
    "# Third axis contains the DataFrame of the different seeds\n",
    "validation_sets = []\n",
    "test_sets = []\n",
    "zeros_shot_datasets = []\n",
    "\n",
    "\n",
    "for folder in sorted(os.listdir(experimient_dir)):\n",
    "    log_dir = os.path.join(experimient_dir, folder)\n",
    "    if os.path.isdir(log_dir):\n",
    "        save_path = log_dir + '/results/'\n",
    "        # Call the event_to_df function with the log directory \n",
    "        val, test, zero= read_csv_from_dir(log_dir,REPO_PATH)\n",
    "        #validation_sets.append(val)\n",
    "        test_sets.append(test)\n",
    "        zeros_shot_datasets.append(zero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 4, 4]\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['geo_strongly_balanced',\n",
       " 'geo_unbalanced',\n",
       " 'geo_weakly_balanced',\n",
       " 'mixed_strongly_balanced',\n",
       " 'mixed_weakly_balanced']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([len(val) for val in test_sets])\n",
    "print(len(test_sets))\n",
    "sorted(os.listdir(experimient_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Strongly Balanced', 'Unbalanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Weakly Balanced']\n",
    "save_path = '/share/temp/bjordan/good_practices_in_machine_learning/good_practices_ml/finetuning/runs/figures/'\n",
    "for i, experiment in enumerate(test_sets):\n",
    "    name = dataset_names[i]\n",
    "    compare_loss(experiment, name, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_test_sets = [sublist[0] for sublist in test_sets]\n",
    "for dataframe in l1_test_sets:\n",
    "    return\n",
    "    compare_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_sets_with_region = [df.filter(like='region') for df in validation_sets]\n",
    "#validation_sets_with_country = [df.filter(like='country') for df in validation_sets]\n",
    "\n",
    "test_sets_with_region = [[df.filter(like='region') for df in sub_array] for sub_array in test_sets]\n",
    "test_sets_with_country = [[df.filter(like='country') for df in sub_array] for sub_array in test_sets]\n",
    "\n",
    "zero_shot_sets_with_region = [[df.filter(like='region') for df in sub_array] for sub_array in zeros_shot_datasets]\n",
    "zero_shot_sets_with_country = [[df.filter(like='country') for df in sub_array] for sub_array in zeros_shot_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Strongly Balanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Weakly Balanced']\n",
    "#box_plot_experiments(validation_sets_with_region, 'Validation Region', save_path, loss_number=0, dataset_names=['geo_strongly_balanced', 'geo_unbalanced', 'geo_weakly_balanced'], metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "#box_plot_experiments(validation_sets_with_country, 'Validation Country', save_path, loss_number=0, dataset_names=['geo_strongly_balanced', 'geo_unbalanced', 'geo_weakly_balanced'], metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "\n",
    "box_plot_experiments(test_sets_with_region, 'Regions on Test-Set', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "box_plot_experiments(test_sets_with_country, 'Countries on Test-Set', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "\n",
    "box_plot_experiments(zero_shot_sets_with_region, 'Zero Shot Regions', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "box_plot_experiments(zero_shot_sets_with_country, 'Zero Shot Countries', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(os.listdir(experimient_dir)))\n",
    "\n",
    "for experiment in test_sets:\n",
    "    buffer = []\n",
    "    for i, df in enumerate(experiment):\n",
    "        buffer.append(df.assign(Experiment=f'L{i+1}').drop(columns=['prediction', 'label', 'ignored_classes', 'ignored_regions']))\n",
    "    buffer = pd.concat(buffer)\n",
    "    print(buffer.groupby('Experiment').mean().round(decimals=3).style.highlight_max(props='textbf:--rwrap;').format(precision=3).to_latex())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      " & ignored_classes & ignored_regions \\\\\n",
      "Experiment &  &  \\\\\n",
      "L1 & 50.800 & 4.600 \\\\\n",
      "L2 & 59.100 & 5.700 \\\\\n",
      "L3 & \\textbf{61.700} & 5.700 \\\\\n",
      "L4 & 58.400 & \\textbf{6.700} \\\\\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrr}\n",
      " & ignored_classes & ignored_regions \\\\\n",
      "Experiment &  &  \\\\\n",
      "L1 & \\textbf{100.000} & \\textbf{18.000} \\\\\n",
      "L2 & \\textbf{100.000} & \\textbf{18.000} \\\\\n",
      "L3 & \\textbf{100.000} & \\textbf{18.000} \\\\\n",
      "L4 & \\textbf{100.000} & \\textbf{18.000} \\\\\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrr}\n",
      " & ignored_classes & ignored_regions \\\\\n",
      "Experiment &  &  \\\\\n",
      "L1 & 73.100 & 7.100 \\\\\n",
      "L2 & 77.500 & \\textbf{8.000} \\\\\n",
      "L3 & \\textbf{82.000} & 7.600 \\\\\n",
      "L4 & 75.800 & \\textbf{8.000} \\\\\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrr}\n",
      " & ignored_classes & ignored_regions \\\\\n",
      "Experiment &  &  \\\\\n",
      "L1 & 50.600 & 5.100 \\\\\n",
      "L2 & 55.600 & 5.700 \\\\\n",
      "L3 & \\textbf{64.800} & 5.400 \\\\\n",
      "L4 & 54.500 & \\textbf{5.900} \\\\\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrr}\n",
      " & ignored_classes & ignored_regions \\\\\n",
      "Experiment &  &  \\\\\n",
      "L1 & 72.900 & 6.800 \\\\\n",
      "L2 & 76.500 & 7.800 \\\\\n",
      "L3 & \\textbf{80.400} & 8.300 \\\\\n",
      "L4 & 75.900 & \\textbf{8.400} \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ignored_regions</th>\n",
       "      <th>Experiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Mixed Strongly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Mixed Weakly Balanced</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ignored_regions               Experiment\n",
       "0                5        Strongly Balanced\n",
       "0                4        Strongly Balanced\n",
       "0                4        Strongly Balanced\n",
       "0                4        Strongly Balanced\n",
       "0                5        Strongly Balanced\n",
       "0                3        Strongly Balanced\n",
       "0                4        Strongly Balanced\n",
       "0                7        Strongly Balanced\n",
       "0                5        Strongly Balanced\n",
       "0                5        Strongly Balanced\n",
       "0                7          Weakly Balanced\n",
       "0                7          Weakly Balanced\n",
       "0                8          Weakly Balanced\n",
       "0                8          Weakly Balanced\n",
       "0                7          Weakly Balanced\n",
       "0                5          Weakly Balanced\n",
       "0                6          Weakly Balanced\n",
       "0                7          Weakly Balanced\n",
       "0                9          Weakly Balanced\n",
       "0                7          Weakly Balanced\n",
       "0                5  Mixed Strongly Balanced\n",
       "0                8  Mixed Strongly Balanced\n",
       "0                5  Mixed Strongly Balanced\n",
       "0                6  Mixed Strongly Balanced\n",
       "0                6  Mixed Strongly Balanced\n",
       "0                5  Mixed Strongly Balanced\n",
       "0                3  Mixed Strongly Balanced\n",
       "0                4  Mixed Strongly Balanced\n",
       "0                5  Mixed Strongly Balanced\n",
       "0                4  Mixed Strongly Balanced\n",
       "0                9    Mixed Weakly Balanced\n",
       "0                7    Mixed Weakly Balanced\n",
       "0                6    Mixed Weakly Balanced\n",
       "0                7    Mixed Weakly Balanced\n",
       "0                5    Mixed Weakly Balanced\n",
       "0                6    Mixed Weakly Balanced\n",
       "0                8    Mixed Weakly Balanced\n",
       "0                8    Mixed Weakly Balanced\n",
       "0                6    Mixed Weakly Balanced\n",
       "0                6    Mixed Weakly Balanced"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for experiment in test_sets:\n",
    "    buffer = []\n",
    "    for i, df in enumerate(experiment):\n",
    "        buffer.append(df.assign(Experiment=f'L{i+1}')[['ignored_classes', 'ignored_regions', 'Experiment']])\n",
    "    buffer = pd.concat(buffer)\n",
    "    print(buffer.groupby('Experiment').mean().round(decimals=3).style.highlight_max(props='textbf:--rwrap;').format(precision=3).to_latex())\n",
    "box_plot_experiments(test_sets, 'Ignored Classes on Test Set', save_path, loss_number=0, metric_names=['ignored_classes'], dataset_names=dataset_names, legend_out_of_plot=True)\n",
    "box_plot_experiments(test_sets, 'Ignored Regions on Test Set', save_path, loss_number=0, metric_names=['ignored_regions'], dataset_names=dataset_names, legend_out_of_plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConfusionMatrix(true_countries, predicted_countries, figure_label, REPO_PATH):\n",
    "        \"\"\"\n",
    "        Creates and visualizes the confusion matrix for country and region predictions.\n",
    "\n",
    "        Args:\n",
    "            true_countries (list): List of true country labels.\n",
    "            predicted_countries (list): List of predicted country labels.\n",
    "            figure_label (str): Label for the generated figures.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        country_list = f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv'\n",
    "        country_list = pd.read_csv(country_list)\n",
    "        regional_ordering_index = [8, 11, 144, 3, 4, 12, 16, 26, 28, 44, 46, 51, 52, 66, 74, 83, 95, 101, 105, 109, 121, 128, 153, 180, 191, 201, 202, 32, 43, 77, 81, 134, 140, 146, 179, 99, 106, 185, 187, 198, 58, 98, 122, 131, 133, 136, 159, 163, 166, 177, 178, 193, 195, 209, 210, 41, 80, 97, 102, 103, 126, 127, 192, 20, 31, 48, 84, 119, 152, 160, 162, 173, 194, 60, 137, 149, 165, 204, 78, 156, 7, 34, 35, 40, 64, 53, 56, 116, 117, 167, 188, 23, 33, 72, 196, 13, 50, 55, 59, 62, 65, 69,\n",
    "                                        86, 88, 92, 94, 113, 115, 142, 168, 172, 38, 148, 189, 205, 9, 25, 27, 39, 42, 54, 61, 68, 76, 79, 147, 157, 197, 200, 24, 85, 100, 107, 125, 135, 150, 169, 184, 186, 203, 30, 138, 182, 208, 2, 17, 29, 89, 91, 111, 132, 143, 151, 0, 5, 15, 57, 71, 75, 82, 93, 120, 123, 130, 155, 161, 171, 175, 199, 206, 19, 22, 37, 45, 70, 73, 112, 124, 129, 139, 170, 174, 176, 183, 1, 6, 14, 21, 47, 67, 87, 90, 96, 104, 108, 145, 154, 158, 164, 181, 190, 207, 10, 18, 36, 49, 63, 110, 114, 118, 141]\n",
    "        # constant for classes\n",
    "        classes = country_list['Country']\n",
    "        np_classes = np.array(classes)\n",
    "\n",
    "        # Build country confusion matrix\n",
    "        cf_matrix = confusion_matrix(\n",
    "            true_countries, predicted_countries, labels=range(0, 211))\n",
    "        ordered_index = np.argsort(-cf_matrix.diagonal())\n",
    "        ordered_matrix = cf_matrix[ordered_index][:, ordered_index]\n",
    "\n",
    "        regionally_ordered_matrix = cf_matrix[regional_ordering_index][:,\n",
    "                                                                            regional_ordering_index]\n",
    "\n",
    "        ordered_classes = np_classes[ordered_index]\n",
    "        regionally_ordered_classes = np_classes[regional_ordering_index]\n",
    "\n",
    "        df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
    "        ordered_df_cm = pd.DataFrame(\n",
    "            ordered_matrix, index=ordered_classes, columns=ordered_classes)\n",
    "        regionally_ordered_df_cm = pd.DataFrame(\n",
    "            regionally_ordered_matrix, index=regionally_ordered_classes, columns=regionally_ordered_classes)\n",
    "\n",
    "        np_regions = np.sort(\n",
    "            np.array(list(set(country_list['Intermediate Region Name']))))\n",
    "\n",
    "        # Build region confusion matrix\n",
    "        true_regions = []\n",
    "        predicted_regions = []\n",
    "        for i in range(0, len(true_countries)):\n",
    "            true_regions.append(ast.literal_eval(\n",
    "                country_list.iloc[true_countries[i]][\"One Hot Region\"]).index(1))\n",
    "            predicted_regions.append(ast.literal_eval(\n",
    "                country_list.iloc[predicted_countries[i]][\"One Hot Region\"]).index(1))\n",
    "\n",
    "        regions_cf_matrix = confusion_matrix(\n",
    "            true_regions, predicted_regions, labels=range(0, len(np_regions)))\n",
    "        regions_ordered_index = np.argsort(-regions_cf_matrix.diagonal())\n",
    "        regions_ordered_matrix = regions_cf_matrix[regions_ordered_index][:,\n",
    "                                                                          regions_ordered_index]\n",
    "\n",
    "        ordered_regions = np_regions[regions_ordered_index]\n",
    "\n",
    "        regions_df_cm = pd.DataFrame(\n",
    "            regions_cf_matrix, index=np_regions, columns=np_regions)\n",
    "        regions_ordered_df_cm = pd.DataFrame(\n",
    "            regions_ordered_matrix, index=ordered_regions, columns=ordered_regions)\n",
    "        plt.figure(1, figsize=(120, 70))\n",
    "        figure = sn.heatmap(df_cm, cmap=sn.cubehelix_palette(\n",
    "            as_cmap=True)).get_figure()\n",
    "        plt.figure(2, figsize=(120, 70))\n",
    "        ordered_figure = sn.heatmap(\n",
    "            ordered_df_cm, cmap=sn.cubehelix_palette(as_cmap=True)).get_figure()\n",
    "        plt.figure(3, figsize=(120, 70))\n",
    "        regionally_ordered_figure = sn.heatmap(\n",
    "            regionally_ordered_df_cm, cmap=sn.cubehelix_palette(as_cmap=True)).get_figure()\n",
    "        plt.figure(4, figsize=(120, 70))\n",
    "        regions_figure = sn.heatmap(\n",
    "            regions_df_cm, cmap=sn.cubehelix_palette(as_cmap=True)).get_figure()\n",
    "        plt.figure(5, figsize=(120, 70))\n",
    "        regions_ordered_figure = sn.heatmap(\n",
    "            regions_ordered_df_cm, cmap=sn.cubehelix_palette(as_cmap=True)).get_figure()\n",
    "        figure.savefig(f'{REPO_PATH}/figures/{figure_label}_country.png')\n",
    "        ordered_figure.savefig(save_path)\n",
    "        regionally_ordered_figure.savefig(save_path)\n",
    "        regions_figure.savefig(save_path)\n",
    "        regions_ordered_figure.savefig(save_path)\n",
    "        plt.close(figure)\n",
    "        plt.close(ordered_figure)\n",
    "        plt.close(regionally_ordered_figure)\n",
    "        plt.close(regions_figure)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
