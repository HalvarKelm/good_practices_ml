{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.analyzation_tools import corrected_repeated_kFold_cv_test as cv_test\n",
    "import utils.confusion_matrix as cm\n",
    "from finetuning.model.region_loss import Regional_Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_loss(list_of_df, name, save_path):\n",
    "    \"\"\"\n",
    "    Function to compare metrics of different experiments using t-tests.\n",
    "    :param list_of_df: List of dataframes\n",
    "    :param name: Name of the experiment\n",
    "    :param save_path: Path to save the plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    list_of_df = [df.copy() for df in list_of_df]\n",
    "    for i in range(len(list_of_df)):\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=f\"L{i+1}\")\n",
    "        cols_to_drop= list_of_df[i].filter(like='text', axis=1).columns.tolist()\n",
    "        cols_to_drop.extend(['prediction', 'label'])\n",
    "        list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "        \n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    metrics = condf.columns[:-1]\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: x.item() if isinstance(x, torch.Tensor) else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if isinstance(x, list) else x)\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].astype(float)\n",
    "    loss_config = ['L1', 'L2', 'L3', 'L4']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        matrix = []\n",
    "        values_matrix = []\n",
    "        print(f\"Metric: {metric}\")\n",
    "        for i in range(len(loss_config)):\n",
    "            signi_buffer = []\n",
    "            value_buffer = []\n",
    "            for j in range(len(loss_config)):\n",
    "                exp1 = loss_config[i]\n",
    "                exp2 = loss_config[j]\n",
    "\n",
    "                values1 = meltdf[meltdf['Experiment'] == exp1]\n",
    "                values1 = values1[values1['Metric'] == metric]['Value']\n",
    "                values2 = meltdf[meltdf['Experiment'] == exp2]\n",
    "                values2 = values2[values2['Metric'] == metric]['Value']\n",
    "                # We assume significnce level of 0.05; due to 10 flod validation we have 9:1 ration of samples\n",
    "                ttest_result = ttest_ind(values1.to_list(), values2.to_list())\n",
    "                t_stat = ttest_result.statistic\n",
    "                p_value = ttest_result.pvalue\n",
    "                degrees_of_freedom = len(values1.to_list()) + len(values2.to_list()) - 2\n",
    "                if p_value < 0.05:\n",
    "                    if values1.mean() > values2.mean():\n",
    "                        print(f\"{exp1} is significantly better than {exp2}\")\n",
    "                        signi_buffer.append(1)\n",
    "                    else:\n",
    "                        print(f\"{exp2} is significantly better than {exp1}\")\n",
    "                        signi_buffer.append(-1)\n",
    "                else:\n",
    "                    print(f\"No significant difference between {exp1} and {exp2}\")\n",
    "                    signi_buffer.append(0)\n",
    "                value_str = (f\"M1={values1.mean():.3f}, S1={values1.std():.3f}\\n\"\n",
    "                             f\"M2={values2.mean():.3f}, S2={values2.std():.3f}\\n\"\n",
    "                             f\"t({degrees_of_freedom}) = {t_stat:.3f}, p={p_value:.3f}\")\n",
    "                value_buffer.append(value_str)\n",
    "                \n",
    "            matrix.append(signi_buffer)\n",
    "            values_matrix.append(value_buffer)\n",
    "            \n",
    "        matrix = pd.DataFrame(matrix, index=loss_config, columns=loss_config)\n",
    "        values_matrix = pd.DataFrame(values_matrix, index=loss_config, columns=loss_config)\n",
    "        \n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(matrix, annot=values_matrix, fmt='', cmap='coolwarm', cbar=False)\n",
    "        plt.title(f\"{metric} comparison\")\n",
    "        plt.savefig(save_path + f\"{name}_{metric}_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_dataframe(df_data1, df_data2, dataset_names):\n",
    "    \"\"\"\n",
    "    Function to compare metrics of different experiments using t-tests.\n",
    "    :param list_of_df: List of dataframes\n",
    "    :param name: Name of the experiment\n",
    "    :param save_path: Path to save the plot\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    cols_to_drop = df_data1.filter(like='text', axis=1).columns.tolist()\n",
    "    cols_to_drop.extend(['prediction', 'label'])\n",
    "    df_data1 = df_data1.drop(columns=cols_to_drop)\n",
    "    df_data2 = df_data2.drop(columns=cols_to_drop)\n",
    "    df_data1.columns = df_data1.columns.str.split().str[-2:].str.join(\" \")\n",
    "    df_data2.columns = df_data2.columns.str.split().str[-2:].str.join(\" \")\n",
    "    metrics = df_data1.columns[:-1]\n",
    "        \n",
    "    for metric in metrics:\n",
    "        print(f\"Metric: {metric}\")\n",
    "        data1 = df_data1[metric]\n",
    "        data2 = df_data2[metric]\n",
    "        t_stat, p_value = ttest_ind(data1.to_list(), data2.to_list())\n",
    "        degrees_of_freedom = len(data1.to_list()) + len(data2.to_list()) - 2\n",
    "        if p_value < 0.05:\n",
    "            if data1.mean() > data2.mean():\n",
    "                print(f\"{dataset_names[0]} is significantly better than {dataset_names[1]}\")\n",
    "            else:\n",
    "                print(f\"{dataset_names[1]} is significantly better than {dataset_names[0]}\")\n",
    "        else:\n",
    "            print(f\"No significant difference between {dataset_names[0]} and {dataset_names[1]}\")\n",
    "        print(f\"data1 = {dataset_names[0]}, data2={dataset_names[1]}\")\n",
    "        print(f\"M1={data1.mean():.3f}, S1={data1.std():.3f}\\n M2={data2.mean():.3f}, S2={data2.std():.3f}\\n t({degrees_of_freedom}) = {t_stat:.3f}, p={p_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_metrics(df, data_type, REPO_PATH):\n",
    "    \"\"\"\n",
    "    Calculate the metrics for region and country columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the metrics.\n",
    "        data_type (str): The type of data (validation or test).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    country_list = f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv'\n",
    "    country_list = pd.read_csv(country_list)\n",
    "    metrics_calculator = Regional_Loss(country_list=country_list)\n",
    "    # Convert the 'Output' column to a list of tensors\n",
    "    df['Output'] = df['Output'].apply(lambda x: torch.tensor(x))\n",
    "    if data_type == 'validation':\n",
    "        dfs = np.array_split(df, 10)\n",
    "        for df in dfs:\n",
    "            # Stack the list of tensors into a single tensor\n",
    "            outputs = torch.stack(df['Output'].tolist())\n",
    "            c_ac = metrics_calculator.calculate_country_accuracy(outputs, df['Label'])\n",
    "            c_prec, c_rec, c_f1,_,_ = metrics_calculator.calculate_metrics_per_class(outputs, df['Label'])\n",
    "            r_ac = metrics_calculator.claculate_region_accuracy(outputs, df['Label'])\n",
    "            r_prec, r_rec, r_f1,_,_ = metrics_calculator.calculate_metrics_per_region(outputs, df['Label'])\n",
    "            m_prec, m_rec, m_f1,_,_ = metrics_calculator.calculate_mixed_metrics(outputs, df['Label'])\n",
    "            ignored_class = len(df['Label'].unique())  - len(df['Prediction'].unique())\n",
    "            ignored_region= sum(1 for x, y in zip(r_prec, r_rec) if x == 0 and y == 0)\n",
    "\n",
    "            metrics = {\n",
    "                'country_accuracy': [c_ac],\n",
    "                'country_precision': [c_prec.mean()],\n",
    "                'country_recall': [c_rec.mean()],\n",
    "                'country_f1': [c_f1.mean()],\n",
    "                'region_accuracy': [r_ac],\n",
    "                'region_precision': [r_prec.mean()],\n",
    "                'region_recall': [r_rec.mean()],\n",
    "                'region_f1': [r_f1.mean()],\n",
    "                'mixed_precision': [m_prec.mean()],\n",
    "                'mixed_recall': [m_rec.mean()],\n",
    "                'mixed_f1': [m_f1.mean()],\n",
    "                'ignored_classes': [ignored_class],\n",
    "                'ignored_regions': [ignored_region],\n",
    "                'prediction': [df['Prediction']],\n",
    "                'label': [df['Label']]\n",
    "            }\n",
    "            if 'all_metrics' in locals():\n",
    "                all_metrics = pd.concat([all_metrics, pd.DataFrame(metrics)])\n",
    "            else:\n",
    "                all_metrics = pd.DataFrame(metrics)\n",
    "    else:\n",
    "        # Stack the list of tensors into a single tensor\n",
    "        outputs = torch.stack(df['Output'].tolist())\n",
    "        c_ac = metrics_calculator.calculate_country_accuracy(outputs, df['Label'])\n",
    "        c_prec, c_rec, c_f1,_,_ = metrics_calculator.calculate_metrics_per_class(outputs, df['Label'])\n",
    "        r_ac = metrics_calculator.claculate_region_accuracy(outputs, df['Label'])\n",
    "        r_prec, r_rec, r_f1,_,_ = metrics_calculator.calculate_metrics_per_region(outputs, df['Label'])\n",
    "        m_prec, m_rec, m_f1 = metrics_calculator.calculate_mixed_metrics(outputs, df['Label'])\n",
    "        ignored_class = len(df['Label'].unique())  - len(df['Prediction'].unique())\n",
    "        ignored_region= sum(1 for x, y in zip(r_prec, r_rec) if x == 0 and y == 0)\n",
    "\n",
    "        metrics = {\n",
    "            'country_accuracy': [c_ac],\n",
    "            'country_precision': [c_prec.mean()],\n",
    "            'country_recall': [c_rec.mean()],\n",
    "            'country_f1': [c_f1.mean()],\n",
    "            'region_accuracy': [r_ac],\n",
    "            'region_precision': [r_prec.mean()],\n",
    "            'region_recall': [r_rec.mean()],\n",
    "            'region_f1': [r_f1.mean()],\n",
    "            'mixed_precision': [m_prec.mean()],\n",
    "            'mixed_recall': [m_rec.mean()],\n",
    "            'mixed_f1': [m_f1.mean()],\n",
    "            'ignored_classes': [ignored_class],\n",
    "            'ignored_regions': [ignored_region],\n",
    "            'prediction': [df['Prediction']],\n",
    "            'label': [df['Label']]\n",
    "        }\n",
    "        all_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "    return all_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_dir(log_dir, REPO_PATH):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the dataframes\n",
    "    validation_dfs = []\n",
    "    test_dfs = []\n",
    "    zero_shot_dfs = []\n",
    "\n",
    "    # Iterate over the folders in the log directory\n",
    "    for folder in sorted(os.listdir(log_dir)):\n",
    "        folder_path = os.path.join(log_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # calculate the metrics for all seeds in the folder\n",
    "            log_files = glob.glob(folder_path + \"/*\")\n",
    "            validation_buffer = []\n",
    "            test_buffer = []\n",
    "            zero_shot_buffer = []\n",
    "            for file_path in log_files:\n",
    "                if '.csv' not in file_path:\n",
    "                    continue\n",
    "                df = pd.read_csv(file_path,converters={\"Output\": ast.literal_eval})\n",
    "\n",
    "                # Split the data into validation and test data\n",
    "                #if 'validation' in file_path:\n",
    "                #    df = calculate_metrics(df, 'validation', REPO_PATH=REPO_PATH)\n",
    "                #    validation_buffer.append(df)\n",
    "                if 'zero' in file_path:\n",
    "                    df = calculate_metrics(df, 'zero', REPO_PATH=REPO_PATH)\n",
    "                    zero_shot_buffer.append(df)\n",
    "                elif 'test' in file_path:\n",
    "                    df = calculate_metrics(df, 'test', REPO_PATH=REPO_PATH)\n",
    "                    test_buffer.append(df)\n",
    "            #validation_dfs.append(pd.concat(validation_buffer))\n",
    "            test_dfs.append(pd.concat(test_buffer))\n",
    "            zero_shot_dfs.append(pd.concat(zero_shot_buffer))\n",
    "    return validation_dfs, test_dfs, zero_shot_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_experiments(list_of_df, name, save_path, loss_number=0, dataset_names=None, metric_names=None, legend_out_of_plot=False):\n",
    "    \"\"\"\n",
    "    Genreates box plots for all metrics contained in the dataframes.\n",
    "    Compares these metrics for each dataframe in the list.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_df (list): A list of dataframes.\n",
    "    name (str): The name of the experiment.\n",
    "    save_path (str): The path to save the plot.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A concatenated dataframe containing all data with a coloumn tagging the used Loss.\n",
    "    \"\"\"\n",
    "    dataset_to_indices = {'Strongly Balanced':0, 'Unbalanced':1, 'Weakly Balanced':2, 'Mixed Strongly Balanced':3, 'Mixed Weakly Balanced':4}\n",
    "    if dataset_names is not None:\n",
    "        indices = [dataset_to_indices[name] for name in dataset_names]\n",
    "        list_of_df = [list_of_df[i] for i in indices]\n",
    "\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    list_of_df = [df[loss_number].copy() for df in list_of_df]\n",
    "\n",
    "    cols_to_use = metric_names.copy()\n",
    "    cols_to_use.append('Experiment')\n",
    "\n",
    "    for i in range(len(list_of_df)):\n",
    "\n",
    "\n",
    "        keys = list(dataset_to_indices.keys())\n",
    "        list_of_df[i] = list_of_df[i].assign(Experiment=keys[indices[i]])\n",
    "        list_of_df[i] = list_of_df[i][cols_to_use]\n",
    "        #cols_to_drop = list_of_df[i].filter(like='text', axis=1).columns\n",
    "        #list_of_df[i] = list_of_df[i].drop(columns=cols_to_drop)\n",
    "\n",
    "        list_of_df[i].columns = list_of_df[i].columns.str.split().str[-2:].str.join(\" \")\n",
    "        list_of_df[i] = list_of_df[i].rename(columns={'country_accuracy': 'Accuracy', 'country_precision': 'Precision', 'country_recall': 'Recall', 'country_f1': 'F1', 'region_accuracy': 'Accuracy', 'region_precision': 'Precision', 'region_recall': 'Recall', 'region_f1': 'F1', 'mixed_precision': 'Precision', 'mixed_recall': 'Recall', 'mixed_f1': 'F1'})\n",
    "\n",
    "    condf = pd.concat(list_of_df)\n",
    "    meltdf = condf.melt(id_vars=[\"Experiment\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x[0]) if isinstance(x,list) else x) \n",
    "    meltdf[\"Value\"] = meltdf[\"Value\"].apply(lambda x: float(x.item()) if isinstance(x,torch.Tensor) else x) \n",
    "\n",
    "    ax = sns.boxplot(\n",
    "        x=\"Metric\", y=\"Value\", hue=\"Experiment\", data=meltdf, showfliers=False\n",
    "    )\n",
    "    ax.set_title(name)\n",
    "    if legend_out_of_plot:\n",
    "        lgd = plt.legend(loc='upper left', fontsize='small', borderaxespad=0.0, bbox_to_anchor=(1, 1))\n",
    "    else:\n",
    "        lgd = plt.legend(loc='upper right', fontsize='small', borderaxespad=0.0)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=10)\n",
    "    plt.savefig(\n",
    "        save_path + f\"{name}-boxplot.png\",\n",
    "        bbox_extra_artists=(lgd,),\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    return condf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_PATH = '/share/temp/bjordan/good_practices_in_machine_learning/good_practices_ml/'\n",
    "\n",
    "# directory of all experiments\n",
    "experimient_dir= '/share/temp/bjordan/good_practices_in_machine_learning/good_practices_ml/finetuning/runs/merged_seeds2/'\n",
    "# create lists that contain the dataframes of the different experiments\n",
    "# First axis contains the different dataset configurations\n",
    "# Second axis contains the different Loss configurations\n",
    "# Third axis contains the DataFrame of the different seeds\n",
    "validation_sets = []\n",
    "test_sets = []\n",
    "zeros_shot_datasets = []\n",
    "\n",
    "\n",
    "for folder in sorted(os.listdir(experimient_dir)):\n",
    "    log_dir = os.path.join(experimient_dir, folder)\n",
    "    if os.path.isdir(log_dir):\n",
    "        save_path = log_dir + '/results/'\n",
    "        # Call the event_to_df function with the log directory \n",
    "        val, test, zero= read_csv_from_dir(log_dir,REPO_PATH)\n",
    "        #validation_sets.append(val)\n",
    "        test_sets.append(test)\n",
    "        zeros_shot_datasets.append(zero)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([len(val) for val in test_sets])\n",
    "print(len(test_sets))\n",
    "sorted(os.listdir(experimient_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Strongly Balanced', 'Unbalanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Weakly Balanced']\n",
    "save_path = '/share/temp/bjordan/good_practices_in_machine_learning/good_practices_ml/finetuning/runs/new_figures/'\n",
    "for i, experiment in enumerate(test_sets):\n",
    "    name = dataset_names[i]\n",
    "    compare_loss(experiment, name, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_test_sets = [sublist[0] for sublist in test_sets]\n",
    "l1_geo_strongly_balanced = l1_test_sets[0]\n",
    "l1_geo_weakly_balanced = l1_test_sets[2]\n",
    "l1_geo_mixed_strongly_balanced = l1_test_sets[3]\n",
    "l1_geo_mixed_weakly_balanced = l1_test_sets[4]\n",
    "compare_dataframe(l1_geo_strongly_balanced, l1_geo_mixed_strongly_balanced, ['Strongly Balanced', 'Mixed Strongly Balanced'])\n",
    "compare_dataframe(l1_geo_weakly_balanced, l1_geo_mixed_weakly_balanced, ['Weakly Balanced', 'Mixed Weakly Balanced'])\n",
    "compare_dataframe(l1_geo_strongly_balanced, l1_geo_weakly_balanced, ['Strongly Balanced', 'Weakly Balanced'])\n",
    "compare_dataframe(l1_geo_mixed_strongly_balanced, l1_geo_mixed_weakly_balanced, ['Mixed Strongly Balanced', 'Mixed Weakly Balanced'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_sets_with_region = [df.filter(like='region') for df in validation_sets]\n",
    "#validation_sets_with_country = [df.filter(like='country') for df in validation_sets]\n",
    "\n",
    "test_sets_with_region = [[df.filter(like='region') for df in sub_array] for sub_array in test_sets]\n",
    "test_sets_with_country = [[df.filter(like='country') for df in sub_array] for sub_array in test_sets]\n",
    "test_sets_mixed = [[df.filter(like='mixed') for df in sub_array] for sub_array in test_sets]\n",
    "\n",
    "zero_shot_sets_with_region = [[df.filter(like='region') for df in sub_array] for sub_array in zeros_shot_datasets]\n",
    "zero_shot_sets_with_country = [[df.filter(like='country') for df in sub_array] for sub_array in zeros_shot_datasets]\n",
    "zero_shot_sets_mixed = [[df.filter(like='mixed') for df in sub_array] for sub_array in zeros_shot_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Strongly Balanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Weakly Balanced']\n",
    "#box_plot_experiments(validation_sets_with_region, 'Validation Region', save_path, loss_number=0, dataset_names=['geo_strongly_balanced', 'geo_unbalanced', 'geo_weakly_balanced'], metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "#box_plot_experiments(validation_sets_with_country, 'Validation Country', save_path, loss_number=0, dataset_names=['geo_strongly_balanced', 'geo_unbalanced', 'geo_weakly_balanced'], metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "\n",
    "box_plot_experiments(test_sets_with_region, 'Regions on Test-Set', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "box_plot_experiments(test_sets_with_country, 'Countries on Test-Set', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "box_plot_experiments(test_sets_mixed, 'Mixed on Test-Set', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['mixed_accuracy', 'mixed_precision', 'mixed_recall', 'mixed_f1'], legend_out_of_plot=True)\n",
    "\n",
    "box_plot_experiments(zero_shot_sets_with_region, 'Zero Shot Regions', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['region_accuracy', 'region_precision', 'region_recall', 'region_f1'])\n",
    "box_plot_experiments(zero_shot_sets_with_country, 'Zero Shot Countries', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['country_accuracy', 'country_precision', 'country_recall', 'country_f1'])\n",
    "box_plot_experiments(zero_shot_sets_mixed, 'Zero Shot Mixed', save_path, loss_number=0, dataset_names=dataset_names, metric_names=['mixed_accuracy', 'mixed_precision', 'mixed_recall', 'mixed_f1'], legend_out_of_plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(os.listdir(experimient_dir)))\n",
    "\n",
    "for experiment in test_sets:\n",
    "    buffer = []\n",
    "    for i, df in enumerate(experiment):\n",
    "        buffer.append(df.assign(Experiment=f'L{i+1}').drop(columns=['prediction', 'label', 'ignored_classes', 'ignored_regions', 'mixed_accuracy', 'mixed_precision', 'mixed_recall', 'mixed_f1']))\n",
    "    buffer = pd.concat(buffer)\n",
    "    print(buffer.groupby('Experiment').mean().round(decimals=3).style.highlight_max(props='textbf:--rwrap;').format(precision=3).to_latex())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(os.listdir(experimient_dir)))\n",
    "\n",
    "for experiment in test_sets:\n",
    "    buffer = []\n",
    "    for i, df in enumerate(experiment):\n",
    "        buffer.append(df.assign(Experiment=f'L{i+1}')['mixed_accuracy', 'mixed_precision', 'mixed_recall', 'mixed_f1'])\n",
    "    buffer = pd.concat(buffer)\n",
    "    print(buffer.groupby('Experiment').mean().round(decimals=3).style.highlight_max(props='textbf:--rwrap;').format(precision=3).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in test_sets:\n",
    "    buffer = []\n",
    "    for i, df in enumerate(experiment):\n",
    "        buffer.append(df.assign(Experiment=f'L{i+1}')[['ignored_classes', 'ignored_regions', 'Experiment']])\n",
    "    buffer = pd.concat(buffer)\n",
    "    print(buffer.groupby('Experiment').mean().round(decimals=3).style.highlight_max(props='textbf:--rwrap;').format(precision=3).to_latex())\n",
    "box_plot_experiments(test_sets, 'Ignored Classes on Test Set', save_path, loss_number=0, metric_names=['ignored_classes'], dataset_names=dataset_names, legend_out_of_plot=True)\n",
    "box_plot_experiments(test_sets, 'Ignored Regions on Test Set', save_path, loss_number=0, metric_names=['ignored_regions'], dataset_names=dataset_names, legend_out_of_plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "def create_and_save_confusion_matrices(REPO_PATH, SAVE_FIGURES_PATH, true_countries, predicted_countries, normalize=False):\n",
    "    \"\"\"\n",
    "    Create and save confusion matrices for countries and regions.\n",
    "\n",
    "    Args:\n",
    "        REPO_PATH (str): path to repo folder.\n",
    "        SAVE_FIGURES_PATH (str): path to save the confusion matrices.\n",
    "        true_countries (list): list of true country labels.\n",
    "        predicted_countries (list): list of predicted country labels.\n",
    "        normalize (bool): whether to normalize the confusion matrices or not.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Load country list and regional ordering index\n",
    "    country_list = pd.read_csv(f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv')\n",
    "    regional_ordering_index = [8, 11, 144, 3, 4, 12, 16, 26, 28, 44, 46, 51, 52, 66, 74, 83, 95, 101, 105, 109, 121, 128, 153, 180, 191, 201, 202, 32, 43, 77, 81, 134, 140, 146, 179, 99, 106, 185, 187, 198, 58, 98, 122, 131, 133, 136, 159, 163, 166, 177, 178, 193, 195, 209, 210, 41, 80, 97, 102, 103, 126, 127, 192, 20, 31, 48, 84, 119, 152, 160, 162, 173, 194, 60, 137, 149, 165, 204, 78, 156, 7, 34, 35, 40, 64, 53, 56, 116, 117, 167, 188, 23, 33, 72, 196, 13, 50, 55, 59, 62, 65, 69,\n",
    "                                    86, 88, 92, 94, 113, 115, 142, 168, 172, 38, 148, 189, 205, 9, 25, 27, 39, 42, 54, 61, 68, 76, 79, 147, 157, 197, 200, 24, 85, 100, 107, 125, 135, 150, 169, 184, 186, 203, 30, 138, 182, 208, 2, 17, 29, 89, 91, 111, 132, 143, 151, 0, 5, 15, 57, 71, 75, 82, 93, 120, 123, 130, 155, 161, 171, 175, 199, 206, 19, 22, 37, 45, 70, 73, 112, 124, 129, 139, 170, 174, 176, 183, 1, 6, 14, 21, 47, 67, 87, 90, 96, 104, 108, 145, 154, 158, 164, 181, 190, 207, 10, 18, 36, 49, 63, 110, 114, 118, 141]\n",
    "    \n",
    "    # constant for classes\n",
    "    classes = country_list['Country']\n",
    "    np_classes = np.array(classes)\n",
    "    country_dict = {country: index for index, country in enumerate(country_list[\"Country\"])}\n",
    "\n",
    "    # Build country confusion matrices\n",
    "    #if normalize:\n",
    "    #    cf_matrix = confusion_matrix(true_countries, predicted_countries, labels=range(0, 211), normalize='true')\n",
    "    #else:\n",
    "    #    cf_matrix = confusion_matrix(true_countries, predicted_countries, labels=range(0, 211))\n",
    "    \n",
    "    # Get the unique classes from the 'prediction' and 'label' columns\n",
    "    filtered_classes = list(set([*true_countries, *predicted_countries]))\n",
    "    class_indices = [country_dict[country] for country in filtered_classes]\n",
    "    regional_ordering_index = [x for x in regional_ordering_index if x in class_indices]\n",
    "    true_countries_indices = [country_dict[country] for country in true_countries]\n",
    "    predicted_countries_indices = [country_dict[country] for country in predicted_countries]\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cf_matrix = confusion_matrix(true_countries, predicted_countries, labels=classes, normalize='true')\n",
    "    else:\n",
    "        cf_matrix = confusion_matrix(true_countries, predicted_countries, labels=classes)\n",
    "    ordered_index = np.argsort(-cf_matrix.diagonal())\n",
    "    ordered_index = [x for x in ordered_index if x in class_indices]\n",
    "    ordered_matrix = cf_matrix[ordered_index][:, ordered_index]\n",
    "    regionally_ordered_matrix = cf_matrix[regional_ordering_index][:,regional_ordering_index]\n",
    "    ordered_classes = np_classes[ordered_index]\n",
    "    regionally_ordered_classes = np_classes[regional_ordering_index]\n",
    "\n",
    "    df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
    "    ordered_df_cm = pd.DataFrame(\n",
    "        ordered_matrix, index=ordered_classes, columns=ordered_classes)\n",
    "    regionally_ordered_df_cm = pd.DataFrame(\n",
    "        regionally_ordered_matrix, index=regionally_ordered_classes, columns=regionally_ordered_classes)\n",
    "\n",
    "    #Create region labels\n",
    "    np_regions = np.sort(np.array(list(set(country_list['Intermediate Region Name']))))\n",
    "    true_regions = []\n",
    "    true_regions_indices = []\n",
    "    predicted_regions = []\n",
    "    predicted_regions_indices = []\n",
    "    for i in range(0, len(true_countries)):\n",
    "        true_country_index = country_dict[true_countries[i]]\n",
    "        predicted_country_index = country_dict[predicted_countries[i]]\n",
    "        true_regions.append(country_list.iloc[true_country_index][\"Intermediate Region Name\"])\n",
    "        predicted_regions.append(country_list.iloc[predicted_country_index][\"Intermediate Region Name\"])\n",
    "        true_regions_indices.append(ast.literal_eval(country_list.iloc[true_countries_indices[i]][\"One Hot Region\"]).index(1))\n",
    "        predicted_regions_indices.append(ast.literal_eval(country_list.iloc[predicted_countries_indices[i]][\"One Hot Region\"]).index(1))\n",
    "\n",
    "    region_indices = list(set([*true_regions_indices, *predicted_regions_indices]))\n",
    "\n",
    "    # Build region confusion matrices\n",
    "    #if normalize:\n",
    "    #    regions_cf_matrix = confusion_matrix(true_regions, predicted_regions, labels=range(0, len(np_regions)), normalize='true')\n",
    "    #else:\n",
    "    #    regions_cf_matrix = confusion_matrix(true_regions, predicted_regions, labels=range(0, len(np_regions)))\n",
    "    if normalize:\n",
    "        regions_cf_matrix = confusion_matrix(true_regions, predicted_regions, labels=np_regions, normalize='true')\n",
    "    else:\n",
    "        regions_cf_matrix = confusion_matrix(true_regions, predicted_regions, labels=np_regions)\n",
    "    regions_ordered_index = np.argsort(-regions_cf_matrix.diagonal())\n",
    "    regions_ordered_index = [x for x in regions_ordered_index if x in region_indices]\n",
    "    regions_ordered_matrix = regions_cf_matrix[regions_ordered_index][:,regions_ordered_index]\n",
    "    ordered_regions = np_regions[regions_ordered_index]\n",
    "\n",
    "    regions_df_cm = pd.DataFrame(regions_cf_matrix, index=np_regions, columns=np_regions)\n",
    "    regions_ordered_df_cm = pd.DataFrame(regions_ordered_matrix, index=ordered_regions, columns=ordered_regions)\n",
    "\n",
    "    # Save confusion matrices\n",
    "    if normalize:\n",
    "        if not os.path.exists(f'{SAVE_FIGURES_PATH}/normalized'):\n",
    "            os.makedirs(f'{SAVE_FIGURES_PATH}/normalized')\n",
    "    else:\n",
    "        if not os.path.exists(f'{SAVE_FIGURES_PATH}'):\n",
    "            os.makedirs(f'{SAVE_FIGURES_PATH}')       \n",
    "\n",
    "    fig_1, ax_1 = plt.subplots(figsize=(120, 90))\n",
    "    sns.set(font_scale=8)\n",
    "    ax_1 = sns.heatmap(df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels = 1,yticklabels=1)\n",
    "    ax_1.tick_params(axis='both', labelsize=15)\n",
    "    ax_1.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_1.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/simple_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_1.figure.savefig(f'{SAVE_FIGURES_PATH}/simple_confusion_matrix.png')\n",
    "    fig_2, ax_2 = plt.subplots(figsize=(120, 90))\n",
    "    ax_2 = sns.heatmap(ordered_df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels=1,yticklabels=1)\n",
    "    ax_2.tick_params(axis='both', labelsize=15)\n",
    "    ax_2.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_2.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/ordered_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_2.figure.savefig(f'{SAVE_FIGURES_PATH}/ordered_confusion_matrix.png')\n",
    "    fig_3, ax_3 = plt.subplots(figsize=(120, 90))\n",
    "    ax_3 = sns.heatmap(regionally_ordered_df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels=1,yticklabels=1)\n",
    "    ax_3.tick_params(axis='both', labelsize=15)\n",
    "    ax_3.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_3.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/regionally_ordered_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_3.figure.savefig(f'{SAVE_FIGURES_PATH}/regionally_ordered_confusion_matrix.png')\n",
    "    fig_4, ax_4 = plt.subplots(figsize=(120, 90))\n",
    "    ax_4 = sns.heatmap(regions_df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels = 1,yticklabels=1)\n",
    "    ax_4.tick_params(axis='both', labelsize=50)\n",
    "    ax_4.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_4.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/regions_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_4.figure.savefig(f'{SAVE_FIGURES_PATH}/regions_confusion_matrix.png')\n",
    "    fig_5, ax_5 = plt.subplots(figsize=(120, 90))\n",
    "    ax_5 = sns.heatmap(regions_ordered_df_cm, cmap=sns.cubehelix_palette(as_cmap=True),xticklabels = 1,yticklabels=1)\n",
    "    ax_5.tick_params(axis='both', labelsize=50)\n",
    "    ax_5.set(xlabel=None, ylabel=None)\n",
    "    if normalize:\n",
    "        ax_5.figure.savefig(f'{SAVE_FIGURES_PATH}/normalized/regions_ordered_confusion_matrix.png')\n",
    "    else:\n",
    "        ax_5.figure.savefig(f'{SAVE_FIGURES_PATH}/regions_ordered_confusion_matrix.png')\n",
    "    fig_1.clf()\n",
    "    fig_2.clf()\n",
    "    fig_3.clf()\n",
    "    fig_4.clf()\n",
    "    fig_5.clf()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(l1_test_sets):\n",
    "    matrix_save_path = save_path + dataset_names[i] + '/'\n",
    "    true_countries = pd.concat(dataset['label'].tolist())\n",
    "    print(true_countries)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Strongly Balanced', 'Unbalanced', 'Weakly Balanced', 'Mixed Strongly Balanced', 'Mixed Weakly Balanced']\n",
    "dataset_to_indices = {'Strongly Balanced':0, 'Unbalanced':1, 'Weakly Balanced':2, 'Mixed Strongly Balanced':3, 'Mixed Weakly Balanced':4}\n",
    "country_list = pd.read_csv(f'{REPO_PATH}/utils/country_list/country_list_region_and_continent.csv')\n",
    "country_dict = {country: index for index, country in enumerate(country_list[\"Country\"])}\n",
    "\n",
    "\n",
    "\n",
    "for i, dataset in enumerate(l1_test_sets):\n",
    "\n",
    "    matrix_save_path = save_path + dataset_names[i] + '/'\n",
    "    true_countries = pd.concat(dataset['label'].tolist()).values\n",
    "    predicted_countries = pd.concat(dataset['prediction'].tolist()).values\n",
    "    #true_countries = true_countries.map(country_dict)\n",
    "    #predicted_countries = predicted_countries.map(country_dict)\n",
    "    create_and_save_confusion_matrices(REPO_PATH=REPO_PATH, SAVE_FIGURES_PATH=matrix_save_path, true_countries=true_countries, predicted_countries=predicted_countries, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
